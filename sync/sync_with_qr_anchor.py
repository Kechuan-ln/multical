#!/usr/bin/env python3
"""
åŸºäºAnchor QRç è§†é¢‘çš„ç›¸æœºåŒæ­¥å·¥å…· (å¢å¼ºç‰ˆ)

æ ¸å¿ƒç†å¿µ:
- ä½¿ç”¨å·²çŸ¥æ—¶é—´åºåˆ—çš„QRç è§†é¢‘ä½œä¸ºanchorï¼ˆå‚è€ƒåŸºå‡†ï¼‰
- ä¸¤ä¸ªç›¸æœºå½•åˆ¶è¯¥QRç è§†é¢‘ï¼Œå³ä½¿çœ‹åˆ°çš„QRç åºåˆ—ä¸åŒ
- é€šè¿‡anchor timecodeæ˜ å°„ï¼Œè®¡ç®—ä¸¤ä¸ªç›¸æœºçš„ç›¸å¯¹æ—¶é—´åç§»

å·¥ä½œåŸç†:
1. Camera1 åœ¨ t1 æ—¶åˆ»çœ‹åˆ° QRç  #100
2. Camera2 åœ¨ t2 æ—¶åˆ»çœ‹åˆ° QRç  #150
3. ä»anchor metadataå¾—çŸ¥: QR#100 å¯¹åº” anchoræ—¶é—´ T1, QR#150 å¯¹åº” anchoræ—¶é—´ T2
4. è®¡ç®—åç§»: offset = (t1 - T1) - (t2 - T2)
"""

import cv2
import numpy as np
import os
import json
import csv
import argparse
import subprocess
import shutil
import sys
from typing import List, Tuple, Optional, Dict
from pathlib import Path

try:
    from pyzbar import pyzbar
    HAS_PYZBAR = True
except ImportError:
    HAS_PYZBAR = False


def get_ffmpeg_path() -> str:
    """Get ffmpeg path from system PATH or conda environment"""
    # Try system PATH first
    ffmpeg = shutil.which('ffmpeg')
    if ffmpeg:
        return ffmpeg

    # Try conda environment
    if hasattr(sys, 'base_prefix'):
        conda_ffmpeg = os.path.join(os.path.dirname(sys.executable), 'ffmpeg')
        if os.path.exists(conda_ffmpeg):
            return conda_ffmpeg

    # Fallback to 'ffmpeg' and let it fail with clear error
    return 'ffmpeg'


# Get ffmpeg path once at module load
FFMPEG = get_ffmpeg_path()


def detect_qr_fast(frame: np.ndarray) -> List[str]:
    """
    å¿«é€ŸQRæ£€æµ‹ï¼ˆæ”¯æŒpyzbarå’ŒOpenCVåŒå¼•æ“ï¼‰
    """
    if len(frame.shape) == 3:
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    else:
        gray = frame

    # é™é‡‡æ ·åŠ é€Ÿ
    if gray.shape[0] > 1080:
        scale = 1080.0 / gray.shape[0]
        gray = cv2.resize(gray, None, fx=scale, fy=scale,
                         interpolation=cv2.INTER_AREA)

    results = []

    # ä¼˜å…ˆä½¿ç”¨pyzbar
    if HAS_PYZBAR:
        try:
            detected = pyzbar.decode(gray, symbols=[pyzbar.ZBarSymbol.QRCODE])
            if detected:
                for obj in detected:
                    results.append(obj.data.decode('utf-8'))
        except:
            pass

    # å¤‡ç”¨ï¼šOpenCV
    if not results:
        try:
            detector = cv2.QRCodeDetector()
            data, vertices, _ = detector.detectAndDecode(gray)
            if data:
                results.append(data)
        except:
            pass

    return results


def parse_qr_frame_number(qr_data: str, prefix: str = "") -> Optional[int]:
    """
    è§£æQRç ï¼Œæå–å¸§ç¼–å·

    Args:
        qr_data: QRç æ•°æ®ï¼ˆå¦‚"000042"æˆ–"SYNC-000042"ï¼‰
        prefix: é¢„æœŸçš„å‰ç¼€ï¼ˆå¦‚"SYNC-"ï¼‰

    Returns:
        å¸§ç¼–å·ï¼ˆæ•´æ•°ï¼‰ï¼Œå¦‚æœè§£æå¤±è´¥è¿”å›None
    """
    try:
        if prefix and qr_data.startswith(prefix):
            qr_data = qr_data[len(prefix):]
        frame_num = int(qr_data)
        return frame_num
    except:
        return None


def extract_anchor_metadata_from_video(video_path: str,
                                       prefix: str = "",
                                       sample_frames: int = 100,
                                       frame_step: int = 10) -> Tuple[Dict[int, float], float]:
    """
    ä»anchor QRç è§†é¢‘ä¸­æå–metadataï¼ˆè‡ªåŠ¨æ£€æµ‹QRç åºåˆ—ï¼‰

    Args:
        video_path: anchorè§†é¢‘è·¯å¾„
        prefix: QRç å‰ç¼€
        sample_frames: æœ€å¤šé‡‡æ ·çš„å¸§æ•°
        frame_step: é‡‡æ ·æ­¥é•¿

    Returns:
        (anchor_map, detected_fps): anchoræ˜ å°„å­—å…¸å’Œæ£€æµ‹åˆ°çš„FPS
    """
    print(f"ä»anchorè§†é¢‘æå–metadata: {os.path.basename(video_path)}")

    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        raise ValueError(f"æ— æ³•æ‰“å¼€anchorè§†é¢‘: {video_path}")

    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    duration = total_frames / fps if fps > 0 else 0

    print(f"  è§†é¢‘ä¿¡æ¯: {fps:.2f}fps, {duration:.2f}s, {total_frames}å¸§")
    print(f"  é‡‡æ ·ç­–ç•¥: æ¯{frame_step}å¸§é‡‡æ ·ä¸€æ¬¡ï¼Œæœ€å¤š{sample_frames}å¸§")

    anchor_map = {}
    frame_idx = 0
    sampled_count = 0

    while frame_idx < total_frames and sampled_count < sample_frames:
        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
        ret, frame = cap.read()

        if not ret:
            break

        # æ£€æµ‹QRç 
        qr_codes = detect_qr_fast(frame)
        for qr_data in qr_codes:
            qr_frame_num = parse_qr_frame_number(qr_data, prefix)
            if qr_frame_num is not None:
                anchor_time = frame_idx / fps
                anchor_map[qr_frame_num] = anchor_time

                sampled_count += 1
                if sampled_count % 20 == 0:
                    print(f"    å·²é‡‡æ · {sampled_count} ä¸ªQRç ...", end='\r')
                break  # æ¯å¸§åªå–ç¬¬ä¸€ä¸ªQRç 

        frame_idx += frame_step

    cap.release()

    print(f"\n  âœ… æå–äº† {len(anchor_map)} ä¸ªQRç æ˜ å°„")

    if not anchor_map:
        raise ValueError(f"âŒ æ— æ³•ä»anchorè§†é¢‘ä¸­æå–QRç ï¼Œè¯·æ£€æŸ¥è§†é¢‘è´¨é‡")

    # éªŒè¯QRç åºåˆ—çš„è¿ç»­æ€§
    qr_numbers = sorted(anchor_map.keys())
    print(f"  QRç èŒƒå›´: {qr_numbers[0]} - {qr_numbers[-1]}")

    # æ£€æµ‹FPSï¼ˆé€šè¿‡QRç åºåˆ—æ¨æ–­ï¼‰
    if len(qr_numbers) >= 2:
        # è®¡ç®—QRç ç¼–å·å¢é•¿é€Ÿåº¦
        qr_diffs = np.diff(qr_numbers)
        time_diffs = np.diff([anchor_map[qr] for qr in qr_numbers])

        # QRç æ¯ç§’å¢é•¿é€Ÿåº¦ = QRç å¢é‡ / æ—¶é—´å¢é‡
        qr_rates = qr_diffs / time_diffs
        detected_qr_fps = np.median(qr_rates)

        print(f"  æ£€æµ‹åˆ°çš„QRç å¸§ç‡: {detected_qr_fps:.2f} fps")

        # å¦‚æœQRç å¸§ç‡æ¥è¿‘è§†é¢‘FPSï¼Œè¯´æ˜æ˜¯æ ‡å‡†çš„é€å¸§QRç è§†é¢‘
        if abs(detected_qr_fps - fps) < 2.0:
            print(f"  âœ“ QRç åºåˆ—ä¸è§†é¢‘FPSä¸€è‡´ï¼ˆé€å¸§QRç ï¼‰")
        else:
            print(f"  âš ï¸  QRç åºåˆ—FPS ({detected_qr_fps:.2f}) ä¸è§†é¢‘FPS ({fps:.2f}) ä¸åŒ")
            print(f"      å¯èƒ½æ˜¯å¾ªç¯æ’­æ”¾æˆ–éæ ‡å‡†ç”Ÿæˆæ–¹å¼")

    return anchor_map, fps


def load_anchor_metadata(csv_path: Optional[str],
                         video_path: Optional[str],
                         fps: float = 30.0,
                         prefix: str = "") -> Tuple[Optional[Dict[int, float]], float]:
    """
    åŠ è½½anchor QRç è§†é¢‘çš„metadataï¼ˆQRå¸§ç¼–å· -> anchoræ—¶é—´ï¼‰

    ä¼˜å…ˆçº§: CSV > è§†é¢‘æå– > é»˜è®¤å‡è®¾

    Args:
        csv_path: CSVæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        video_path: anchorè§†é¢‘è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        fps: å¦‚æœéƒ½æ²¡æœ‰ï¼Œå‡è®¾anchorè§†é¢‘çš„FPS
        prefix: QRç å‰ç¼€

    Returns:
        (anchor_map, effective_fps): anchoræ˜ å°„å­—å…¸å’Œæœ‰æ•ˆFPS
    """
    # ä¼˜å…ˆçº§1: CSVæ–‡ä»¶
    if csv_path and Path(csv_path).exists():
        print(f"åŠ è½½anchor metadata CSV: {csv_path}")
        anchor_map = {}
        with open(csv_path, 'r') as f:
            reader = csv.DictReader(f)
            for row in reader:
                try:
                    frame_num = int(row.get('frame_number', row.get('frame', 0)))
                    anchor_time = float(row.get('anchor_time', row.get('time', 0)))
                    anchor_map[frame_num] = anchor_time
                except:
                    continue
        print(f"  âœ… åŠ è½½äº† {len(anchor_map)} æ¡anchoræ—¶é—´æ˜ å°„")
        return anchor_map, fps

    # ä¼˜å…ˆçº§2: ä»anchorè§†é¢‘æå–
    if video_path and Path(video_path).exists():
        try:
            anchor_map, detected_fps = extract_anchor_metadata_from_video(
                video_path, prefix, sample_frames=200, frame_step=5
            )
            return anchor_map, detected_fps
        except Exception as e:
            print(f"  âš ï¸  ä»è§†é¢‘æå–å¤±è´¥: {e}")
            print(f"  å›é€€åˆ°é»˜è®¤æ˜ å°„")

    # ä¼˜å…ˆçº§3: é»˜è®¤å‡è®¾
    print(f"ä½¿ç”¨é»˜è®¤anchoræ˜ å°„: frame_number / {fps}")
    return None, fps


def get_anchor_time(qr_frame_num: int,
                    anchor_map: Optional[Dict[int, float]],
                    fps: float = 30.0) -> float:
    """
    è·å–QRå¸§ç¼–å·å¯¹åº”çš„anchoræ—¶é—´

    Args:
        qr_frame_num: QRç å¸§ç¼–å·
        anchor_map: anchoræ—¶é—´æ˜ å°„å­—å…¸ï¼ˆå¯ä¸ºNoneï¼‰
        fps: é»˜è®¤FPSï¼ˆå½“anchor_mapä¸ºNoneæ—¶ä½¿ç”¨ï¼‰

    Returns:
        anchoræ—¶é—´ï¼ˆç§’ï¼‰
    """
    if anchor_map is not None:
        return anchor_map.get(qr_frame_num, qr_frame_num / fps)
    else:
        return qr_frame_num / fps


def scan_video_qr_segment(video_path: str,
                          start_time: float = 0.0,
                          duration: float = 60.0,
                          frame_step: int = 5,
                          prefix: str = "") -> List[Tuple[float, int]]:
    """
    æ‰«æè§†é¢‘ç‰‡æ®µä¸­çš„QRç 

    Args:
        video_path: è§†é¢‘è·¯å¾„
        start_time: å¼€å§‹æ—¶é—´ï¼ˆç§’ï¼‰
        duration: æ‰«ææ—¶é•¿ï¼ˆç§’ï¼‰
        frame_step: å¸§é—´éš”
        prefix: QRç å‰ç¼€

    Returns:
        [(video_time, qr_frame_number), ...] åˆ—è¡¨
    """
    print(f"æ‰«æ: {os.path.basename(video_path)}")
    print(f"  æ—¶é—´æ®µ: {start_time:.1f}s - {start_time + duration:.1f}s")

    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print(f"  âŒ æ— æ³•æ‰“å¼€è§†é¢‘")
        return []

    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    video_duration = total_frames / fps if fps > 0 else 0

    print(f"  è§†é¢‘ä¿¡æ¯: {fps:.2f}fps, {video_duration:.2f}s, {total_frames}å¸§")

    start_frame = int(start_time * fps)
    end_frame = min(int((start_time + duration) * fps), total_frames)

    if start_frame >= total_frames:
        print(f"  âš ï¸ èµ·å§‹æ—¶é—´è¶…å‡ºè§†é¢‘èŒƒå›´")
        cap.release()
        return []

    cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)

    detections = []
    seen_qr_frames = {}

    frame_idx = start_frame
    scan_count = 0

    while frame_idx < end_frame:
        ret, frame = cap.read()
        if not ret:
            break

        if (frame_idx - start_frame) % frame_step == 0:
            scan_count += 1
            qr_codes = detect_qr_fast(frame)

            for qr_data in qr_codes:
                qr_frame_num = parse_qr_frame_number(qr_data, prefix)
                if qr_frame_num is not None:
                    video_time = frame_idx / fps
                    if qr_frame_num not in seen_qr_frames:
                        seen_qr_frames[qr_frame_num] = []
                    seen_qr_frames[qr_frame_num].append(video_time)

        frame_idx += 1

        if scan_count % 200 == 0:
            print(f"    è¿›åº¦: {frame_idx - start_frame}/{end_frame - start_frame}",
                  end='\r')

    cap.release()

    # å–ä¸­ä½æ•°
    for qr_frame_num, times in seen_qr_frames.items():
        median_time = np.median(times)
        detections.append((median_time, qr_frame_num))

    detections.sort()

    print(f"\n  âœ… æ£€æµ‹åˆ° {len(detections)} ä¸ªå”¯ä¸€QRç  (æ‰«æäº†{scan_count}å¸§)")
    if detections:
        qr_range = f"{detections[0][1]} - {detections[-1][1]}"
        print(f"  QRç èŒƒå›´: {qr_range}")

    return detections


def calculate_sync_offset_with_anchor(
        video1_detections: List[Tuple[float, int]],
        video2_detections: List[Tuple[float, int]],
        anchor_map: Optional[Dict[int, float]],
        anchor_fps: float = 30.0) -> Optional[Dict]:
    """
    é€šè¿‡anchor timecodeæ˜ å°„è®¡ç®—åŒæ­¥åç§»

    åŸç†:
    - Video1åœ¨æ—¶åˆ»t1çœ‹åˆ°QR#N1ï¼Œå¯¹åº”anchoræ—¶é—´T1
    - Video2åœ¨æ—¶åˆ»t2çœ‹åˆ°QR#N2ï¼Œå¯¹åº”anchoræ—¶é—´T2
    - Video1ç›¸å¯¹anchorçš„åç§»: offset1 = t1 - T1
    - Video2ç›¸å¯¹anchorçš„åç§»: offset2 = t2 - T2
    - ä¸¤ä¸ªè§†é¢‘çš„ç›¸å¯¹åç§»: offset = offset1 - offset2

    Args:
        video1_detections: è§†é¢‘1çš„æ£€æµ‹ç»“æœ [(video_time, qr_frame_num), ...]
        video2_detections: è§†é¢‘2çš„æ£€æµ‹ç»“æœ
        anchor_map: anchoræ—¶é—´æ˜ å°„
        anchor_fps: anchor FPS

    Returns:
        åŒæ­¥ç»“æœå­—å…¸ï¼ŒåŒ…å«offsetã€ç»Ÿè®¡ä¿¡æ¯ç­‰
    """
    if not video1_detections or not video2_detections:
        print("âŒ è‡³å°‘ä¸€ä¸ªè§†é¢‘æ²¡æœ‰æ£€æµ‹åˆ°QRç ")
        return None

    print("\nè®¡ç®—åŒæ­¥åç§»ï¼ˆåŸºäºanchor timecodeï¼‰...")

    # 1. å°†æ¯ä¸ªæ£€æµ‹æ˜ å°„åˆ°anchoræ—¶é—´
    video1_pairs = []  # [(video_time, anchor_time), ...]
    video2_pairs = []

    for video_time, qr_frame_num in video1_detections:
        anchor_time = get_anchor_time(qr_frame_num, anchor_map, anchor_fps)
        video1_pairs.append((video_time, anchor_time, qr_frame_num))

    for video_time, qr_frame_num in video2_detections:
        anchor_time = get_anchor_time(qr_frame_num, anchor_map, anchor_fps)
        video2_pairs.append((video_time, anchor_time, qr_frame_num))

    print(f"  Video1: {len(video1_pairs)} ä¸ªQRç æ˜ å°„")
    print(f"  Video2: {len(video2_pairs)} ä¸ªQRç æ˜ å°„")

    # 2. è®¡ç®—æ¯ä¸ªè§†é¢‘ç›¸å¯¹anchorçš„åç§»
    video1_offsets = [(vt - at, qr) for vt, at, qr in video1_pairs]
    video2_offsets = [(vt - at, qr) for vt, at, qr in video2_pairs]

    # 3. ä½¿ç”¨ä¸­ä½æ•°ä¼°è®¡æ¯ä¸ªè§†é¢‘çš„anchoråç§»ï¼ˆé²æ£’æ€§ï¼‰
    video1_offset_median = np.median([off for off, _ in video1_offsets])
    video2_offset_median = np.median([off for off, _ in video2_offsets])

    print(f"  Video1ç›¸å¯¹anchoråç§»: {video1_offset_median:.3f}s")
    print(f"  Video2ç›¸å¯¹anchoråç§»: {video2_offset_median:.3f}s")

    # 4. è®¡ç®—ç›¸å¯¹åç§»
    relative_offset = video1_offset_median - video2_offset_median

    print(f"  ç›¸å¯¹åç§» (Video1 - Video2): {relative_offset:.3f}s")

    # 5. éªŒè¯ä¸€è‡´æ€§ï¼ˆæ£€æŸ¥ç¦»ç¾¤ç‚¹ï¼‰
    video1_std = np.std([off for off, _ in video1_offsets])
    video2_std = np.std([off for off, _ in video2_offsets])

    print(f"  Video1åç§»æ ‡å‡†å·®: {video1_std:.3f}s")
    print(f"  Video2åç§»æ ‡å‡†å·®: {video2_std:.3f}s")

    if video1_std > 0.5 or video2_std > 0.5:
        print(f"  âš ï¸ è­¦å‘Š: åç§»æ ‡å‡†å·®è¾ƒå¤§ï¼Œå¯èƒ½å­˜åœ¨æ—¶é—´æ¼‚ç§»æˆ–æ£€æµ‹é”™è¯¯")

    # 6. å¯è§†åŒ–QRç æ˜ å°„ï¼ˆå‰10ä¸ªï¼‰
    print("\n  QRç æ˜ å°„ç¤ºä¾‹ï¼ˆå‰10ä¸ªï¼‰:")
    print("  Video1:")
    for i, (vt, at, qr) in enumerate(video1_pairs[:10]):
        print(f"    [{i+1}] QR#{qr:06d}: video_t={vt:.2f}s, anchor_t={at:.2f}s, offset={vt-at:.3f}s")

    print("  Video2:")
    for i, (vt, at, qr) in enumerate(video2_pairs[:10]):
        print(f"    [{i+1}] QR#{qr:06d}: video_t={vt:.2f}s, anchor_t={at:.2f}s, offset={vt-at:.3f}s")

    result = {
        "offset_seconds": float(relative_offset),
        "video1_anchor_offset": float(video1_offset_median),
        "video2_anchor_offset": float(video2_offset_median),
        "video1_offset_std": float(video1_std),
        "video2_offset_std": float(video2_std),
        "video1_qr_count": len(video1_pairs),
        "video2_qr_count": len(video2_pairs),
        "video1_qr_range": [int(video1_pairs[0][2]), int(video1_pairs[-1][2])],
        "video2_qr_range": [int(video2_pairs[0][2]), int(video2_pairs[-1][2])],
    }

    return result


def get_video_info(video_path: str) -> Dict:
    """è·å–è§†é¢‘ä¿¡æ¯"""
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        raise ValueError(f"æ— æ³•æ‰“å¼€è§†é¢‘: {video_path}")

    fps = cap.get(cv2.CAP_PROP_FPS)
    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    duration = frame_count / fps if fps > 0 else 0

    cap.release()

    return {
        "fps": fps,
        "frame_count": frame_count,
        "width": width,
        "height": height,
        "duration": duration
    }


def create_stacked_video(video1_path: str, video2_path: str, output_path: str,
                         layout: str = "hstack", duration: float = 10.0) -> bool:
    """
    åˆ›å»ºstackedå¯¹æ¯”è§†é¢‘ï¼ˆç”¨äºéªŒè¯åŒæ­¥æ•ˆæœï¼‰

    Args:
        video1_path: è§†é¢‘1è·¯å¾„
        video2_path: è§†é¢‘2è·¯å¾„ï¼ˆå·²åŒæ­¥ï¼‰
        output_path: è¾“å‡ºè·¯å¾„
        layout: å¸ƒå±€æ–¹å¼ ("hstack"=å·¦å³, "vstack"=ä¸Šä¸‹)
        duration: è¾“å‡ºæ—¶é•¿ï¼ˆç§’ï¼‰

    Returns:
        æ˜¯å¦æˆåŠŸ
    """
    print(f"\nåˆ›å»ºStackedå¯¹æ¯”è§†é¢‘...")
    print(f"  å¸ƒå±€: {layout}")
    print(f"  æ—¶é•¿: {duration:.1f}s")

    # è·å–è§†é¢‘ä¿¡æ¯
    video1_info = get_video_info(video1_path)
    video2_info = get_video_info(video2_path)

    # ä½¿ç”¨è¾ƒä½çš„åˆ†è¾¨ç‡ä»¥åŠ å¿«å¤„ç†
    scale_width = 960  # ç¼©æ”¾åˆ°960å®½åº¦

    if layout == "hstack":
        # å·¦å³æ‹¼æ¥
        filter_complex = (
            f"[0:v]scale={scale_width}:-1[v0];"
            f"[1:v]scale={scale_width}:-1[v1];"
            f"[v0][v1]hstack=inputs=2"
        )
    else:
        # ä¸Šä¸‹æ‹¼æ¥
        filter_complex = (
            f"[0:v]scale={scale_width}:-1[v0];"
            f"[1:v]scale={scale_width}:-1[v1];"
            f"[v0][v1]vstack=inputs=2"
        )

    cmd = [
        FFMPEG, '-y',
        '-i', video1_path,
        '-i', video2_path,
        '-filter_complex', filter_complex,
        '-t', str(duration),
        '-c:v', 'libx264', '-preset', 'fast', '-crf', '23',
        '-pix_fmt', 'yuv420p',
        output_path
    ]

    result = subprocess.run(cmd, capture_output=True, text=True)

    if result.returncode == 0 and os.path.exists(output_path):
        print(f"  âœ… Stackedè§†é¢‘åˆ›å»ºå®Œæˆ: {output_path}")
        output_info = get_video_info(output_path)
        print(f"  è¾“å‡º: {output_info['width']}x{output_info['height']}, {output_info['duration']:.2f}s")
        return True
    else:
        print(f"  âŒ åˆ›å»ºå¤±è´¥")
        if result.stderr:
            print(f"  é”™è¯¯: {result.stderr[:200]}")
        return False


def create_synced_video(video1_path: str, video2_path: str, output_path: str,
                        offset_seconds: float, target_fps: float) -> bool:
    """
    åˆ›å»ºåŒæ­¥åçš„video2

    Args:
        video1_path: å‚è€ƒè§†é¢‘ï¼ˆVideo1ï¼‰
        video2_path: éœ€è¦åŒæ­¥çš„è§†é¢‘ï¼ˆVideo2ï¼‰
        output_path: è¾“å‡ºè·¯å¾„
        offset_seconds: æ—¶é—´åç§»ï¼ˆæ­£å€¼è¡¨ç¤ºVideo2éœ€è¦å»¶è¿Ÿï¼‰
        target_fps: ç›®æ ‡å¸§ç‡
    """
    print(f"\nåˆ›å»ºåŒæ­¥è§†é¢‘...")
    print(f"  å‚è€ƒ: {os.path.basename(video1_path)}")
    print(f"  åŒæ­¥: {os.path.basename(video2_path)} -> {os.path.basename(output_path)}")
    print(f"  åç§»: {offset_seconds:.3f}s")

    video1_info = get_video_info(video1_path)
    video2_info = get_video_info(video2_path)

    target_duration = video1_info['duration']

    print(f"  ç›®æ ‡: {target_duration:.2f}s @ {target_fps:.2f}fps")

    # æ ¹æ®offsetå†³å®šç­–ç•¥
    if offset_seconds > 0:
        # Video2éœ€è¦å»¶è¿Ÿ -> å‰é¢å¡«å……é»‘å¸§
        black_duration = offset_seconds
        content_duration = target_duration - black_duration

        if content_duration <= 0:
            print(f"  âŒ é”™è¯¯: offsetå¤ªå¤§ï¼Œæ— æ³•åˆ›å»ºåŒæ­¥è§†é¢‘")
            return False

        print(f"  æ–¹æ¡ˆ: å‰é¢å¡«å…… {black_duration:.3f}s é»‘å¸§")

        # åˆ›å»ºé»‘å¸§è§†é¢‘
        black_video = output_path.replace('.mp4', '_black.mp4')
        cmd_black = [
            FFMPEG, '-y',
            '-f', 'lavfi',
            '-i', f'color=c=black:s={video2_info["width"]}x{video2_info["height"]}:r={target_fps}',
            '-t', str(black_duration),
            '-c:v', 'libx264', '-preset', 'ultrafast', '-crf', '0',
            '-pix_fmt', 'yuv420p',
            black_video
        ]

        print(f"  åˆ›å»ºé»‘å¸§è§†é¢‘...")
        subprocess.run(cmd_black, capture_output=True)

        # è°ƒæ•´Video2çš„å¸§ç‡å’Œæ—¶é•¿
        adjusted_video = output_path.replace('.mp4', '_adjusted.mp4')
        vf_str = f'fps={target_fps}'

        cmd_adjust = [
            FFMPEG, '-y',
            '-i', video2_path,
            '-vf', vf_str,
            '-t', str(content_duration),
            '-c:v', 'libx264', '-preset', 'ultrafast', '-crf', '0',
            '-pix_fmt', 'yuv420p',
            adjusted_video
        ]

        print(f"  è°ƒæ•´Video2: {video2_info['duration']:.2f}s @ {video2_info['fps']:.2f}fps -> {content_duration:.2f}s @ {target_fps:.2f}fps")
        subprocess.run(cmd_adjust, capture_output=True)

        # æ‹¼æ¥
        concat_list = output_path.replace('.mp4', '_concat.txt')
        with open(concat_list, 'w') as f:
            f.write(f"file '{os.path.abspath(black_video)}'\n")
            f.write(f"file '{os.path.abspath(adjusted_video)}'\n")

        cmd_concat = [
            FFMPEG, '-y',
            '-f', 'concat', '-safe', '0', '-i', concat_list,
            '-r', str(target_fps),
            '-c:v', 'libx264', '-preset', 'medium', '-crf', '18',
            '-pix_fmt', 'yuv420p',
            output_path
        ]

        print(f"  æ‹¼æ¥æœ€ç»ˆè§†é¢‘...")
        subprocess.run(cmd_concat, capture_output=True)

        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        for temp in [black_video, adjusted_video, concat_list]:
            if os.path.exists(temp):
                os.remove(temp)

    else:
        # Video2éœ€è¦æå‰ -> è£å‰ªå¼€å¤´
        trim_duration = abs(offset_seconds)
        content_duration = min(target_duration, video2_info['duration'] - trim_duration)

        print(f"  æ–¹æ¡ˆ: è£å‰ªå¼€å¤´ {trim_duration:.3f}s")

        vf_str = f'fps={target_fps}'

        cmd = [
            FFMPEG, '-y',
            '-ss', str(trim_duration),
            '-i', video2_path,
            '-vf', vf_str,
            '-t', str(content_duration),
            '-c:v', 'libx264', '-preset', 'medium', '-crf', '18',
            '-pix_fmt', 'yuv420p',
            output_path
        ]

        subprocess.run(cmd, capture_output=True)

    # éªŒè¯è¾“å‡º
    if os.path.exists(output_path):
        output_info = get_video_info(output_path)
        print(f"  âœ… åˆ›å»ºå®Œæˆ")
        print(f"  éªŒè¯è¾“å‡º: {output_info['duration']:.2f}s @ {output_info['fps']:.2f}fps")

        if abs(output_info['duration'] - target_duration) > 0.5:
            print(f"  âš ï¸ è­¦å‘Š: è¾“å‡ºæ—¶é•¿ ({output_info['duration']:.2f}s) ä¸ç›®æ ‡ ({target_duration:.2f}s) ä¸åŒ¹é…")

        return True
    else:
        print(f"  âŒ åˆ›å»ºå¤±è´¥")
        return False


def main():
    parser = argparse.ArgumentParser(
        description='åŸºäºanchor QRç è§†é¢‘çš„ç›¸æœºåŒæ­¥å·¥å…·',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
å·¥ä½œåŸç†:
  1. ç”Ÿæˆanchor QRç è§†é¢‘ï¼ˆä½¿ç”¨generate_qr_sync_video.pyï¼‰
  2. ä¸¤ä¸ªç›¸æœºå½•åˆ¶è¯¥QRç è§†é¢‘ï¼ˆå¯ä»¥ä¸åŒæ—¶å¼€å§‹/ç»“æŸï¼‰
  3. æœ¬å·¥å…·æ£€æµ‹QRç ï¼Œé€šè¿‡anchor timecodeæ˜ å°„è®¡ç®—åç§»

ä½¿ç”¨ç¤ºä¾‹:
  # æ–¹æ³•1: ç›´æ¥ä½¿ç”¨anchorè§†é¢‘ï¼ˆæœ€ç®€å•ï¼Œæ¨èï¼‰
  python sync_with_qr_anchor.py \\
    --video1 camera1.mp4 \\
    --video2 camera2.mp4 \\
    --output camera2_synced.mp4 \\
    --anchor-video qr_anchor.mp4

  # æ–¹æ³•2: æä¾›anchor metadata CSV
  python sync_with_qr_anchor.py \\
    --video1 camera1.mp4 \\
    --video2 camera2.mp4 \\
    --output camera2_synced.mp4 \\
    --anchor-csv qr_metadata.csv

  # æ–¹æ³•3: ä½¿ç”¨é»˜è®¤æ˜ å°„ï¼ˆéœ€è¦çŸ¥é“anchor FPSï¼‰
  python sync_with_qr_anchor.py \\
    --video1 camera1.mp4 \\
    --video2 camera2.mp4 \\
    --output camera2_synced.mp4 \\
    --anchor-fps 30

  # æŒ‡å®šæ‰«æèŒƒå›´å’Œå‰ç¼€
  python sync_with_qr_anchor.py \\
    --video1 camera1.mp4 \\
    --video2 camera2.mp4 \\
    --output camera2_synced.mp4 \\
    --anchor-video qr_anchor.mp4 \\
    --prefix "SYNC-" \\
    --scan-start 5 \\
    --scan-duration 30 \\
    --save-json sync_result.json

  # ç”Ÿæˆstackedå¯¹æ¯”è§†é¢‘ï¼ˆéªŒè¯åŒæ­¥æ•ˆæœï¼‰
  python sync_with_qr_anchor.py \\
    --video1 camera1.mp4 \\
    --video2 camera2.mp4 \\
    --output camera2_synced.mp4 \\
    --anchor-video qr_anchor.mp4 \\
    --stacked verify_sync.mp4 \\
    --stacked-layout hstack \\
    --stacked-duration 15

Anchor CSVæ ¼å¼ï¼ˆå¯é€‰ï¼‰:
  frame_number,anchor_time
  0,0.0
  1,0.033333
  2,0.066667
  ...
        """
    )

    parser.add_argument('--video1', required=True,
                       help='è§†é¢‘1è·¯å¾„ï¼ˆä½œä¸ºå‚è€ƒï¼‰')
    parser.add_argument('--video2', required=True,
                       help='è§†é¢‘2è·¯å¾„ï¼ˆéœ€è¦åŒæ­¥ï¼‰')
    parser.add_argument('--output', required=True,
                       help='è¾“å‡ºåŒæ­¥åçš„è§†é¢‘è·¯å¾„')

    parser.add_argument('--anchor-video', default=None,
                       help='Anchor QRç è§†é¢‘è·¯å¾„ï¼ˆæ¨èï¼Œè‡ªåŠ¨æå–metadataï¼‰')
    parser.add_argument('--anchor-csv', default=None,
                       help='Anchor metadata CSVæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œä¼˜å…ˆçº§é«˜äºè§†é¢‘ï¼‰')
    parser.add_argument('--anchor-fps', type=float, default=30.0,
                       help='Anchorè§†é¢‘FPSï¼ˆé»˜è®¤30ï¼Œä»…åœ¨æ²¡æœ‰è§†é¢‘/CSVæ—¶ä½¿ç”¨ï¼‰')

    parser.add_argument('--scan-start', type=float, default=0.0,
                       help='å¼€å§‹æ‰«ææ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤0')
    parser.add_argument('--scan-duration', type=float, default=30.0,
                       help='æ‰«ææ—¶é•¿ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤30')
    parser.add_argument('--step', type=int, default=5,
                       help='å¸§æ­¥é•¿ï¼ˆæ¯Nå¸§æ£€æµ‹ä¸€æ¬¡ï¼‰ï¼Œé»˜è®¤5')
    parser.add_argument('--prefix', type=str, default='',
                       help='QRç å‰ç¼€ï¼ˆå¦‚"SYNC-"ï¼‰ï¼Œé»˜è®¤æ— ')

    parser.add_argument('--target-fps', type=float, default=None,
                       help='è¾“å‡ºè§†é¢‘FPSï¼ˆé»˜è®¤ä½¿ç”¨video1çš„FPSï¼‰')
    parser.add_argument('--save-json', type=str, default=None,
                       help='ä¿å­˜åŒæ­¥ç»“æœåˆ°JSONæ–‡ä»¶')

    parser.add_argument('--stacked', type=str, default=None,
                       help='ç”Ÿæˆstackedå¯¹æ¯”è§†é¢‘è·¯å¾„ï¼ˆå¯é€‰ï¼Œç”¨äºéªŒè¯åŒæ­¥æ•ˆæœï¼‰')
    parser.add_argument('--stacked-layout', type=str, default='hstack',
                       choices=['hstack', 'vstack'],
                       help='Stackedè§†é¢‘å¸ƒå±€: hstack=å·¦å³, vstack=ä¸Šä¸‹ï¼Œé»˜è®¤hstack')
    parser.add_argument('--stacked-duration', type=float, default=10.0,
                       help='Stackedè§†é¢‘æ—¶é•¿ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤10ç§’')

    args = parser.parse_args()

    # æ£€æŸ¥ä¾èµ–
    if not HAS_PYZBAR:
        print("âš ï¸ è­¦å‘Š: pyzbaræœªå®‰è£…ï¼Œå°†ä½¿ç”¨OpenCVæ£€æµ‹ï¼ˆè¾ƒæ…¢ï¼‰")
        print("   æ¨èå®‰è£…: pip install pyzbar")

    # åŠ è½½anchor metadataï¼ˆä¼˜å…ˆçº§: CSV > è§†é¢‘ > é»˜è®¤ï¼‰
    print("\n" + "=" * 80)
    print("æ­¥éª¤0: åŠ è½½Anchor Metadata")
    print("=" * 80)
    anchor_map, effective_fps = load_anchor_metadata(
        args.anchor_csv, args.anchor_video, args.anchor_fps, args.prefix
    )

    # å¦‚æœä»è§†é¢‘æå–äº†FPSï¼Œæ›´æ–°anchor_fps
    if args.anchor_video and effective_fps != args.anchor_fps:
        print(f"  ä½¿ç”¨æ£€æµ‹åˆ°çš„FPS: {effective_fps:.2f} (è¦†ç›–å‘½ä»¤è¡Œå‚æ•° {args.anchor_fps})")
        args.anchor_fps = effective_fps

    # æ‰«æä¸¤ä¸ªè§†é¢‘
    print("\n" + "=" * 80)
    print("æ­¥éª¤1: æ‰«æVideo1")
    print("=" * 80)
    video1_detections = scan_video_qr_segment(
        args.video1, args.scan_start, args.scan_duration, args.step, args.prefix
    )

    print("\n" + "=" * 80)
    print("æ­¥éª¤2: æ‰«æVideo2")
    print("=" * 80)
    video2_detections = scan_video_qr_segment(
        args.video2, args.scan_start, args.scan_duration, args.step, args.prefix
    )

    # è®¡ç®—åç§»
    print("\n" + "=" * 80)
    print("æ­¥éª¤3: è®¡ç®—åŒæ­¥åç§»")
    print("=" * 80)

    sync_result = calculate_sync_offset_with_anchor(
        video1_detections, video2_detections, anchor_map, args.anchor_fps
    )

    if not sync_result:
        print("\nâŒ åŒæ­¥å¤±è´¥")
        return 1

    # åˆ›å»ºåŒæ­¥è§†é¢‘
    print("\n" + "=" * 80)
    print("æ­¥éª¤4: åˆ›å»ºåŒæ­¥è§†é¢‘")
    print("=" * 80)

    video1_info = get_video_info(args.video1)
    target_fps = args.target_fps if args.target_fps else video1_info['fps']

    success = create_synced_video(
        args.video1, args.video2, args.output,
        sync_result['offset_seconds'], target_fps
    )

    # ä¿å­˜JSONç»“æœ
    if args.save_json:
        result_data = {
            "video1": {
                "path": args.video1,
                "info": get_video_info(args.video1),
                "detections": [[float(t), int(qr)] for t, qr in video1_detections],
            },
            "video2": {
                "path": args.video2,
                "info": get_video_info(args.video2),
                "detections": [[float(t), int(qr)] for t, qr in video2_detections],
            },
            "sync_result": sync_result,
            "output": args.output,
        }

        with open(args.save_json, 'w') as f:
            json.dump(result_data, f, indent=2)

        print(f"\nğŸ’¾ åŒæ­¥ç»“æœå·²ä¿å­˜: {args.save_json}")

    # ç”Ÿæˆstackedå¯¹æ¯”è§†é¢‘ï¼ˆå¯é€‰ï¼‰
    if success and args.stacked:
        print("\n" + "=" * 80)
        print("æ­¥éª¤5: åˆ›å»ºStackedå¯¹æ¯”è§†é¢‘")
        print("=" * 80)

        stacked_success = create_stacked_video(
            args.video1,
            args.output,  # ä½¿ç”¨åŒæ­¥åçš„è§†é¢‘
            args.stacked,
            layout=args.stacked_layout,
            duration=args.stacked_duration
        )

        if stacked_success:
            print(f"  ğŸ’¡ æç¤º: æ’­æ”¾ {args.stacked} æ¥éªŒè¯åŒæ­¥æ•ˆæœ")

    if success:
        print("\n" + "=" * 80)
        print("âœ… åŒæ­¥å®Œæˆï¼")
        print("=" * 80)
        print(f"è¾“å‡º: {args.output}")
        print(f"åç§»: {sync_result['offset_seconds']:.3f}ç§’")
        if args.stacked:
            print(f"å¯¹æ¯”è§†é¢‘: {args.stacked}")
        return 0
    else:
        print("\nâŒ åŒæ­¥è§†é¢‘åˆ›å»ºå¤±è´¥")
        return 1


if __name__ == '__main__':
    exit(main())
